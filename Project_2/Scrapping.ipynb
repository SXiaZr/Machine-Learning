{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping\n",
    "This notebook is written for scrapping paper data of **not dismissed author** from [Web of Science](http://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=F41mtBBV1mNZKmygFN7&preferencesSaved=). (Paper data of **dismissed author** have been scraped by anthor group who collabrates with us in this project.)\n",
    "\n",
    "- The search strategy is searching the full name in **not dismissed author list**.\n",
    "- Since the data scraped from [Web of Science](http://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=F41mtBBV1mNZKmygFN7&preferencesSaved=) is in `.xlsx` format, we use **BeautifulSoup** package to parse it. (The parsing code is stored in file `parsing.py`)\n",
    "- We store those aimed features in `pandas.DataFrame` format: title, uid, publish_date, vol, pubtype, issue, language, doctype, source, keywords, abstract, headings, subheadings, traditional_subjects, extended_subjects, category_info, addresses_info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and parsing package/module \n",
    "from suds.client import Client\n",
    "from scraper_WoS import *\n",
    "from parsing import *\n",
    "\n",
    "# Math packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Other packages\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of copyright and privacy, we can not share it. \n",
    "USER_NAME = 'SWISS10_reproj'\n",
    "PASSWORD = 'Welcome#10 '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_of_author(Querystring):\n",
    "    results = soap.search(Querystring, offset = 1)\n",
    "    if results.recordsFound > 100:\n",
    "        new_records = results.records\n",
    "        for i in range(int(results.recordsFound/100)):\n",
    "            time.sleep(0.5)\n",
    "            research = soap.search(Querystring, offset = (i+1)*100+1)\n",
    "            time.sleep(0.5)\n",
    "            new_records = new_records + research.records\n",
    "        #results = new_records\n",
    "        results.records = new_records\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated (SID: F27iZVmy7KbYkszKRlh)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "F27iZVmy7KbYkszKRlh"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soap = WosClient(user= USER_NAME, password= PASSWORD, lite=False)\n",
    "soap.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eroglu Ilhan',\n",
       " 'Koksal Deniz',\n",
       " 'Yesil Nesibe Karahan',\n",
       " 'Sarpel Tunay',\n",
       " 'k Emel',\n",
       " 'Bariskin Elif',\n",
       " 'Yusufoglu Edagani',\n",
       " 'slani Mahmoud Ali Asghar',\n",
       " 'Veenhof Rob',\n",
       " 'Okutucu Sercan']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of not dismissed author (get from another group)\n",
    "not_dismissed_df = pd.read_excel('./Data/nodismissed_complete.xlsx')\n",
    "# Drop abbreviation\n",
    "not_dismissed_df['Author'] = not_dismissed_df['Author'].map(lambda x:x.lstrip('AU='))\n",
    "not_dismissed_df['Author'] = not_dismissed_df['Author'].map(lambda x: x\\\n",
    "                                                            if (re.sub('\\s+', '', x).isalpha())\\\n",
    "                                                            else np.nan)\n",
    "not_dismissed_df.dropna(inplace=True)\n",
    "not_dismissed_list = not_dismissed_df['Author'].unique()\n",
    "# Drop names which less than 2 words\n",
    "not_dismissed_list = [i for i in not_dismissed_list if len(i.split(' '))>=2]\n",
    "not_dismissed_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AU=Eroglu Ilhan',\n",
       " 'AU=Koksal Deniz',\n",
       " 'AU=Yesil Nesibe Karahan',\n",
       " 'AU=Sarpel Tunay',\n",
       " 'AU=k Emel',\n",
       " 'AU=Bariskin Elif',\n",
       " 'AU=Yusufoglu Edagani',\n",
       " 'AU=slani Mahmoud Ali Asghar',\n",
       " 'AU=Veenhof Rob',\n",
       " 'AU=Okutucu Sercan']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# According to the full names in not dismissed author list, scrape their paper data\n",
    "searchnames_dismiss_auths = []\n",
    "for line in not_dismissed_list: \n",
    "    QueryString = line.strip()\n",
    "    QueryString_author = 'AU=' + QueryString\n",
    "    searchnames_dismiss_auths.append(QueryString_author)\n",
    "searchnames_dismiss_auths[0:10]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dismissed_dataset = pd.DataFrame()\n",
    "for i in range(len(searchnames_dismiss_auths)):\n",
    "        print(i,searchnames_dismiss_auths[i])\n",
    "        if i == 1000: # WOS constraints that each ID can only search for 2500 times. Just set smaller counts to renew the ID.\n",
    "            soap = WosClient(user= 'SWISS10_reproj', password= 'Welcome#10 ', lite=False)\n",
    "            soap.connect()\n",
    "        QueryString = searchnames_dismiss_auths[i]\n",
    "        try:\n",
    "            results = soap.search(QueryString)\n",
    "        except Exception as e:\n",
    "            print('Error on loop',QueryString)\n",
    "            print(e)\n",
    "        if results.recordsFound > 1000:\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        try:\n",
    "            results = pub_of_author(QueryString)\n",
    "            #time.sleep(0.5)\n",
    "            Soup = BeautifulSoup(results.records,'lxml')\n",
    "            dataset = construct_dataset(Soup)\n",
    "            dataset['FullName'] = QueryString.lstrip('AU=')\n",
    "            dismissed_dataset=dismissed_dataset.append(dataset,ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print('Error on loop',QueryString)\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./Data/not_dismissed_dataset.pickle', 'wb') as handle:\n",
    "#     pickle.dump(dismissed_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('./Data/not_dismissed_dataset.pickle', 'rb') as handle:\n",
    "    dismissed_dataset = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
