{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undis_List_Generation_for_FastText\n",
    "\n",
    "This notebook is written to create a list of undismissed researchers extracted from database **author_df_nodup_aff.csv**. The list is used for the generation of feature by FastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math packages\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Other packages\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import collections\n",
    "import gensim\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dismiss = pd.read_csv('./data/dismissed_complete.csv')\n",
    "data_dismiss = data_dismiss.rename(columns = {'subheadind':'subheading'})\n",
    "\n",
    "data_dismiss.drop_duplicates(inplace=True)\n",
    "data_dismiss.dropna(subset=['keywords'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchnames_dismiss_univ = []\n",
    "searchnames_dismiss_fullname = []\n",
    "for line in open('./data/Dismis_Acad_List_CLEAN.txt','r'): \n",
    "    QueryString = line.strip()\n",
    "    QueryString_aff = QueryString.split(',')[1]\n",
    "    QueryString_fullname = QueryString.split(',')[0].split()[-1]+' '+' '.join(QueryString.split(',')[0].split()[0:-1])\n",
    "    searchnames_dismiss_fullname.append(QueryString_fullname)\n",
    "    searchnames_dismiss_univ.append(QueryString_aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auth_aff_df = pd.DataFrame({'Author':searchnames_dismiss_fullname,\n",
    "                            'Affiliation':searchnames_dismiss_univ})\n",
    "auth_names = pd.read_csv('./data/author_df_nodup_aff.csv')\n",
    "auth_names = auth_names[['Author','AbbrNames','title','year','journal','keyword','uid','Affiliation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindAff(paper_df,Abbrname,idx):\n",
    "    tmp_dict = eval(paper_df.iloc[idx].name_info)\n",
    "    tmp_dict = tmp_dict[Abbrname+'*']\n",
    "    if tmp_dict.__contains__('address'):\n",
    "        affliation = tmp_dict['address']['organization']\n",
    "        if affliation == None:\n",
    "            affliation = affliation\n",
    "        else:\n",
    "            affliation = affliation.upper()\n",
    "    elif tmp_dict.__contains__('addr_no_1') & ~tmp_dict.__contains__('addr_no_2'):\n",
    "        affliation = tmp_dict['addr_no_1']['organization']\n",
    "        if affliation == None:\n",
    "            affliation = affliation\n",
    "        else:\n",
    "            affliation = affliation.upper()\n",
    "    else:\n",
    "        affliation = None\n",
    "    return affliation\n",
    "\n",
    "\n",
    "def FindDispName(paper_df,Abbrname,idx):\n",
    "    tmp_dict = eval(paper_df.iloc[idx].name_info)\n",
    "    tmp_dict = tmp_dict[Abbrname+'*']\n",
    "    if tmp_dict.__contains__('display_name'):\n",
    "        disp_name = ' ' .join(list(filter(None,re.split(r\"\\W+\",tmp_dict['display_name']))))\n",
    "        disp_name = disp_name.upper()\n",
    "    else:\n",
    "        disp_name = None\n",
    "    seq_no = int(tmp_dict['seq_no']) if tmp_dict['seq_no'] else None\n",
    "    daisng_id = int(tmp_dict['daisng_id']) if tmp_dict['daisng_id'] else None\n",
    "    return disp_name,seq_no,daisng_id\n",
    "\n",
    "\n",
    "def FindHead (paper_df,idx):\n",
    "    try:\n",
    "        heading_tmp = eval(paper_df.iloc[idx].subheading)\n",
    "        heading = []\n",
    "        for head in heading_tmp:\n",
    "            heading+=(list(filter(None,re.split(r\"\\W+\",head.lower()))))\n",
    "    except:\n",
    "        heading = None\n",
    "    return heading\n",
    "\n",
    "def FindSub (paper_df,idx):\n",
    "    try:\n",
    "        subject_tmp = eval(paper_df.iloc[idx].traditional_sub)\n",
    "        subject = []\n",
    "        for sub in subject_tmp:\n",
    "            subject+=(list(filter(None,re.split(r\"\\W+\",sub.lower()))))\n",
    "    except:\n",
    "        subject = None\n",
    "    return subject\n",
    "\n",
    "\n",
    "def AvgSubVec(sublist,model):\n",
    "    sub2vec = 0\n",
    "    for sub in sublist:\n",
    "        sub2vec += model[sub.lower()]\n",
    "    return sub2vec/len(sublist)\n",
    "\n",
    "\n",
    "def paper_clean(paper_df,Abbrname,NumCluster):\n",
    "    paper_vec_df = pd.DataFrame(columns=['Title','Disp_name','Aff','Sub','seq_no','daisng_id','publish_date','keywords'])\n",
    "    for i in range(len(paper_df)):\n",
    "        Title = paper_df.iloc[i].title\n",
    "        disp_name,seq_no,daisng_id = FindDispName(paper_df,Abbrname,i)\n",
    "        # publish date is added to the frame\n",
    "        pudate = paper_df.iloc[i].publish_date\n",
    "        # keywords added to the frame\n",
    "        keywords = paper_df.iloc[i].keywords\n",
    "        # if no pre-known affiliation then try to find it \n",
    "        Aff = FindAff(paper_df,Abbrname,i)\n",
    "        # if the subject from dataframe exists then try to split it\n",
    "        Heading = None if pd.isnull(paper_df.iloc[i].heading) else FindHead(paper_df,i)\n",
    "        Sub = None if pd.isnull(paper_df.iloc[i].traditional_sub) else FindSub(paper_df,i)\n",
    "        if Heading:\n",
    "            if Sub:\n",
    "                Heading_Sub = Heading + Sub\n",
    "            else:\n",
    "                Heading_Sub = None\n",
    "        else:\n",
    "            Heading_Sub = None\n",
    "        test = pd.DataFrame({'Title':[Title],'Disp_name':[disp_name],'Aff':[Aff],'Sub':[Sub],\\\n",
    "                             'seq_no':seq_no,'daisng_id':daisng_id,'publish_date': pudate,'keywords': keywords})\n",
    "        try:\n",
    "            tmp_kw = eval(paper_df.iloc[i].keywords)\n",
    "        except:\n",
    "            paper_vec_df = paper_vec_df.append(test)\n",
    "            continue\n",
    "        else:\n",
    "            paper_vec_df = paper_vec_df.append(test) \n",
    "    paper_vec_df = paper_vec_df.reset_index(drop=True)\n",
    "    return paper_vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for [j,Abbrname] in enumerate(auth_names.AbbrNames.unique()):\n",
    "    MatchName = '\\''+Abbrname+'\\*'\n",
    "    fullname_search = auth_names.loc[auth_names.AbbrNames.str.contains(Abbrname)][['Author','Affiliation','title','uid','keyword','year']].copy()\n",
    "    NumCluster = fullname_search.Author.unique().shape[0]\n",
    "    auth2cluster = fullname_search.Author.unique()\n",
    "    data_to_cluster = data_dismiss.loc[data_dismiss.AbbrNames.str.contains(MatchName)].copy()\n",
    "    data_to_cluster = data_to_cluster.merge(fullname_search.drop(columns=['title','keyword']),on='uid',how='outer')\n",
    "    data_to_cluster.dropna(subset=['uid'],inplace=True)\n",
    "    data_to_cluster.drop_duplicates(subset=['uid'],inplace=True)\n",
    "    data_to_cluster = data_to_cluster.dropna(subset=['keywords'])\n",
    "    data_to_cluster = data_to_cluster.reset_index(drop=True)\n",
    "    paper_vec = paper_clean(data_to_cluster, Abbrname, NumCluster)\n",
    "    \n",
    "    # HERE PUBLICATIONS BY AUTHORS FROM FOREIGN INSTITUTES ARE REMOVED\n",
    "    for [i,uni] in enumerate(auth_aff_df['Affiliation'].unique()):\n",
    "        if i == 0:\n",
    "            paper_vec_tr = paper_vec.loc[paper_vec['Aff'].values == 'ISTANBUL UNIV']\n",
    "        else:\n",
    "            paper_vec_tr = pd.concat([paper_vec_tr,paper_vec.loc[paper_vec['Aff'].values == uni[:-6]]])\n",
    "    paper_vec = paper_vec_tr\n",
    "    # HERE paper_vec_new_tr REPRESENTS PUBLICATIONS AFTER 2016-01-01\n",
    "    paper_vec_new_tr = paper_vec.loc[paper_vec['publish_date']>'2016-01-01'].copy()\n",
    "    \n",
    "    # HERE PUBLICATIONS BY AMBIGUOUS AUTHORS ARE REMOVED\n",
    "    paper_rm = paper_vec_new_tr.loc[paper_vec_new_tr['Disp_name'].str.contains(Abbrname+' ')]\n",
    "    paper_vec_new_tr = paper_vec_new_tr.drop(index = paper_rm.index,errors = 'ignore')\n",
    "    paper_rm = paper_vec_new_tr.loc[paper_vec_new_tr['Disp_name'].values == Abbrname]\n",
    "    paper_vec_new_tr = paper_vec_new_tr.drop(index = paper_rm.index,errors = 'ignore')\n",
    "    \n",
    "    # HERE PUBLICATIONS BY DISMISSED AUTHORS ARE REMOVED\n",
    "    for cluster_auth in auth2cluster:\n",
    "        paper_rm = paper_vec_new_tr.loc[paper_vec_new_tr['Disp_name'].str.contains(cluster_auth)]\n",
    "        paper_vec_new_tr = paper_vec_new_tr.drop(index = paper_rm.index,errors = 'ignore')\n",
    "        \n",
    "    # HERE y_new IS PUBLICATIONS BY AUTHORS WITH MORE THAN 5 RECORDS\n",
    "    x = paper_vec_new_tr.groupby('Disp_name').count()[['Aff']]\n",
    "    x.columns=['count']\n",
    "    x.reset_index(inplace=True)\n",
    "    y = paper_vec_new_tr.merge(x, on='Disp_name', how='outer')\n",
    "    y_new = y.sort_values(by = 'count')\n",
    "    y_new = y_new.loc[y['count']>5]\n",
    "    \n",
    "    # HERE no_dis_pub IS PUBLICATIONS BY AUTHORS SELECTED\n",
    "    for [i,writer] in enumerate(y_new['Disp_name'].unique()):\n",
    "        if i == 0:\n",
    "            no_dis_pub = paper_vec.loc[paper_vec['Disp_name'] == writer].copy()\n",
    "        else:    \n",
    "            no_dis_pub = pd.concat([no_dis_pub,paper_vec.loc[paper_vec['Disp_name'] == writer].copy()])\n",
    "    if j == 0 :\n",
    "        total_no_dis_pub = no_dis_pub\n",
    "    else :   \n",
    "        total_no_dis_pub = pd.concat([total_no_dis_pub, no_dis_pub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_no_dis_pub = pd.read_csv('./data/total_no_dis_pub_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
